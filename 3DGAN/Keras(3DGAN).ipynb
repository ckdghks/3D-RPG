{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras(3DGAN).ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNJtNUugp24wu2K8EwYPK/X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckdghks/colab-AI/blob/master/Keras(3DGAN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGwV91tlmxuz"
      },
      "source": [
        "#라이브러리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KixqIi4gmvBz"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as io\n",
        "import scipy.ndimage as nd\n",
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import Input\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv3D, Deconv3D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcO7nmYHaky7"
      },
      "source": [
        "#생성기 신경망\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2LcejCnnVci"
      },
      "source": [
        "def build_generator() :\n",
        "    \"\"\"\n",
        "    다음에 정의한 하이퍼파라미터 값들을 사용하는 생성기 모델을 작성한다.\n",
        "    :return: 생성기 신경망\n",
        "    \"\"\"\n",
        "    z_size = 200\n",
        "    gen_filters = [512, 256, 128, 64, 1]\n",
        "    gen_kernel_sizes = [4, 4, 4, 4, 4]\n",
        "    gen_strides = [1, 2, 2, 2, 2]\n",
        "    gen_input_shape = (1, 1, 1, z_size)\n",
        "    gen_activations = ['relu', 'relu', 'relu', 'relu', 'sigmoid']\n",
        "    gen_convolutional_blocks = 5\n",
        "\n",
        "    input_layer = Input(shape=gen_input_shape)\n",
        "\n",
        "    # 첫 번째 3차원 전치합성곱(즉, 3차원 역합성곱) 블록\n",
        "    a = Deconv3D(filters=gen_filters[0],\n",
        "                 kernel_size=gen_kernel_sizes[0],\n",
        "                 strides=gen_strides[0])(input_layer)\n",
        "    a = BatchNormalization()(a, training=True)\n",
        "    a = Activation(activation='relu')(a)\n",
        "\n",
        "    # 그 다음에 이어서 나오는 네 개의 3차원 전치핪어곱 블록록\n",
        "    for i in range(gen_convolutional_blocks - 1):\n",
        "        a = Deconv3D(filters=gen_filters[i + 1],\n",
        "                     kernel_size=gen_kernel_sizes[i + 1],\n",
        "                     strides=gen_strides[i + 1], padding='same')(a)\n",
        "        a = BatchNormalization()(a, training=True)\n",
        "        a = Activation(activation=gen_activations[i + 1])(a)\n",
        "\n",
        "    gen_model = Model(inputs=[input_layer], outputs=[a])\n",
        "    return gen_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZp9rn6ScOmB"
      },
      "source": [
        "#판별기 신경망"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yKfm2npcVS0"
      },
      "source": [
        "def build_discriminator() :\n",
        "    \"\"\"\n",
        "    다음과 같이 정의된 하이퍼파라미터 값들을 사용해 판별기 모델을 만든다.\n",
        "    \"\"\"\n",
        "\n",
        "    dis_input_shape = (64, 64, 64, 1)\n",
        "    dis_filters = [64, 128, 256, 512, 1]\n",
        "    dis_kernel_sizes = [4, 4, 4, 4, 4]\n",
        "    dis_strides = [2, 2, 2, 2, 1]\n",
        "    dis_paddings = ['same', 'same', 'same', 'same', 'valid']\n",
        "    dis_alphas = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
        "    dis_activiations = ['leaky_relu', 'leaky_relu', 'leaky_relu', 'leaky_relu', 'sigmoid']\n",
        "    dis_convolutional_blocks = 5\n",
        "\n",
        "    dis_input_layer = Inpuut(shape=dis_input_shape)\n",
        "\n",
        "    # 첫 번째 3차원 합성곱 블록\n",
        "    a = Conv3D(filters=dis_filters[0]),\n",
        "                        kernel_size=dis_kernel_sizes[0],\n",
        "                        strides=dis_strides[0],\n",
        "                        padding=dis_paddings[0])(dis_input_layer)\n",
        "    a = BatchNormalization()(a, training=True)\n",
        "    a = LeakyReLU(dis_alphas[0])(0)\n",
        "\n",
        "    # 다음으로 추가할 3차원 합성곱 블록 네 개\n",
        "    for i in range(dis_convolutional_blocks - 1):\n",
        "        a = Conv3D(filters=dis_filters[i+1],\n",
        "                            kernel_size=dis_kernel_sizes[i+1],\n",
        "                   strides=dis_strides[i+1],\n",
        "                   padding=dis_paddings[i+1])(a)\n",
        "        a = BatchNormalization()(a, training=True)\n",
        "        if dis_activiations[i+1] == 'leaky_relu':\n",
        "            a = LeakyReLU(dis_alphas[i+1])(a)\n",
        "        elif dis_activiations[i+1] == 'sigmoid':\n",
        "            a = Activation(activaiton='sigmoid')(a)\n",
        "        \n",
        "    dis_model = Model(inputs=dis_input_layer, outputs=a)\n",
        "    print(dis_model.summary())\n",
        "\n",
        "    return dis_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rouP9jytk4Z"
      },
      "source": [
        "#3D-GAN 훈련"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zup-pnydtr8X"
      },
      "source": [
        "##신경망 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJZ7bQkDtyiE"
      },
      "source": [
        "gen_learnig_rater = 0.0025\n",
        "dis_learning_rate = 0.00001\n",
        "beta = 0.5\n",
        "batch_size = 32\n",
        "z_size = 200\n",
        "DIR_PATH = '3DShapenets 데이터셋이 들어있는 디렉터리에 대한 경로'\n",
        "generated_volumes_dir = 'generated_volumes'\n",
        "log_dir = 'logs'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zZM4bDX-YJA"
      },
      "source": [
        "# 인스턴스들을 생성한다.\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu-8hg7w-bzQ"
      },
      "source": [
        "# 최적화기를 지정한다.\n",
        "gen_optimizer = Adam(lr=gen_learning_rate, beta_1=beta)\n",
        "dis_optimizer = Adam(lr=dis_learning_rate, beta_1=0.9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "385rC4Vt-eOP"
      },
      "source": [
        "# 신경망들을 컴파일한다.\n",
        "generator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7aPxquo-hns"
      },
      "source": [
        "discriminator.trainable = False\n",
        "adversarial_model = Sequential()\n",
        "adversarial_model.add(generator)\n",
        "adversarial_model.add(discriminator)\n",
        "adversarial_model.compile(loss=\"binary_crossentropy\",\n",
        "                          optimizer=Adam(lr=gen_learning_rate, beta_1=beta))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtijFDiS_WRa"
      },
      "source": [
        "def getVoxelsFromMat(path, cube_len=64):\n",
        "    voxels = io.loadmat(path)['istance']\n",
        "    voxels = np.pad(voxels, (1, 1), 'constant', constant_values=(0, 0))\n",
        "    if cube_len != 32 and cube_len == 64:\n",
        "        voxels = nd.zoom(voxels, (2, 2, 2), mode='constant', order=0)\n",
        "    return voxels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnjC83EzDOoh"
      },
      "source": [
        "def get3ImagesForACategory(obj='airplane', train=True, cube_len=64, obj_ratio=1.0):\n",
        "    obj_path = DIR_PATH + obj + '/30/'\n",
        "    obj_path += 'train/' if train else 'test/'\n",
        "    fileList = [f for f in os.listdir(obj_path) if f.endswith('.mat')]\n",
        "    fileList = fileList[0:int(obj_ratio * len(fileLIst))]\n",
        "    volumeBatch = np.asarray([getVoxelsFromMat(obj_path + f, cube_len) for f in fileList],\n",
        "                             dtype=np.bool)\n",
        "    return volumeBatch\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKytoUBkFiE7"
      },
      "source": [
        "volumes = get3ImagesForACategory(obj='airplane', train=True, obj_ratio=1.0)\n",
        "volumes = volumes[..., np.newaxis].astype(np.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YiL1K3fGQcq"
      },
      "source": [
        "tensorboard = TensorBoard(log_dir=\"{}/{}\".format(log_dir, time.time()))\n",
        "tensorboard.set_model(generator)\n",
        "tensorboard.set_model(discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usv7NdLeG8YM"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    print(\"Epoch:\", epoch)\n",
        "\n",
        "    # 손실들을 저장할 두 개의 리스트를 만든다.\n",
        "    gen_losses = []\n",
        "    dis_losses = []\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk8OFk1eHO03"
      },
      "source": [
        "number_of_batches = int(volumes.shape[0] / batch_size)\n",
        "print(\"Number of batches:\", number_of_batches)\n",
        "for index in range(numbr_of_batches):\n",
        "    print(\"Batch:\", index + 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H0qXegTHy9W"
      },
      "source": [
        "z_sample = np.random.normal(0, 0.33, size=[batch_size, 1, 1, 1, z_size]).astype(np.float32)\n",
        "volumes_batch = volumes[index * batch_size:(index + 1) * batch_size, :, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76lSFHSDIWqn"
      },
      "source": [
        "gen_volumes = generator.predict(z_sample, verbose=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uY6PgnaKQQG"
      },
      "source": [
        "# 판별기 신경망을 훈련 가능하게 설정한다.\n",
        "discriminator.trainable = True\n",
        "\n",
        "# 가짜 레이블과 진짜 레이블을 생성한다.\n",
        "labels_real = np.reshpae([1] * batch_size, (-1, 1, 1, 1, 1))\n",
        "labels_fake = np.reshape([0] * batch_size, (-1, 1, 1, 1, 1))\n",
        "\n",
        "# 생성기 신경망을 훈련한다.\n",
        "loss_real = discriminator.train_on_batch(volumes_batch, labels_real)\n",
        "loss_fake = discriminator.train_on_batch(gen_volumes, labels_fake)\n",
        "\n",
        "# 총 판별기 손실을 계산한다.\n",
        "d_loss = 0.5 * (loss_real + loss_fake)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E_ORx6vLvQq"
      },
      "source": [
        "z = np.random.normal(0, 0.33, size=[batch_size, 1, 1, 1, z_size]).astype(np.float32)\n",
        "\n",
        "# 적대 모델을 훈련한다.\n",
        "g_loss = adversarial_model.train_on_batch(z, np.reshape([1]* batch_size, (-1, 1, 1, 1, 1)))\n",
        "\n",
        "gen_losses.append(g_loss)\n",
        "dis_losses.append(d_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ul578-ZMUk5"
      },
      "source": [
        "if index % 10 = 0:\n",
        "    z_sample2 = np.random.normal(0, 0.33, size=[batch_size, 1, 1, 1, z_size]).astype(np.float32)\n",
        "    generated_volumes = generator.predict(z_sample2, verbose=3)\n",
        "for i, generated_volume in enumerate(generated_volumes[:5]):\n",
        "    voxels = np.squeese(generated_volume)\n",
        "    voxels[voxels < 0.5] = 0.\n",
        "    voxels[voxels >= 0.5] = 1.\n",
        "    saveFromVoxels(voxels, \"results/img_{}_{}_{}\".format(epoch, index, i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I_gTXMMNYPE"
      },
      "source": [
        "# 텐서보드에 손실들을 저장한다.\n",
        "write_log(tensorboard, 'g_loss', np.mean(gen_losses), epoch)\n",
        "write_log(tensorboard, 'd_loss', np.mean(dis_losses), epoch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
